## 고급 amazon s3  
### amazon S3 - moving between storage classes 스토리지 클래스 간 객체 옮기는 방법

객체를 이전할 수 있는 있다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0e50c9f9-d6b9-4369-b0d8-a40d14ac1451/ff89d7b4-9520-46c5-a04a-b47bd03a8aaa/Untitled.png)

실제로 객체에 자주 액세스하지 않을 거라면 → standard AI로

객체를 아카이브화하려는 거라면 → glacier 티어/ deep archive 티어로

이렇게 객체들을 라이프사이클 규칙을 통해서 자동으로 옮길 수도 있다.

## amazon S3 - lifecycle rules

1. transition action - 다른 스토리지 클래스로 이전하기 위해 객체를 설정한다.
2. expiration actions - 일정한 시간 뒤에 만료되어 객체르 삭제하도록 설정한다.
3. 규칙은 특정 접두어에 대해 지정할 수 있다. → 버킷 전체에 적용하거나 버킷 안의 특정한 경로에 적용할 수 있다.
4. 규칙을 특정 객체 태그에 대해 지정할 수 있다.

### amazon S3 - lifecycle rules - 시나리오1

EC2 애플리케이션이 있고 프로필 사진이 s3에 업로드 된 후에 섬네일을 생성한다.

섬네일은 원본 사진으로부터 쉽게 재생성할 수 있다.

섬네일 보관 기간은 60일, 원본 이미지는 60일 동안은 곧바로 받을 수 있다.

사용자는 최장 6시간 동안 기다릴 수 있다.

⇒ s3 원본 이미지는 standard 클래스에 있을 수 있다. 60일 후에 glacier로 이전하기 위한 라이프사이클 설정이 있다.

⇒ 그리고 섬네일 이미지는 예를 들어 접두어를 사용하여소스와 섬네일을 구분한다.

⇒ 섬네일은 one-zone IA에 있울 수 있다. 빈번히 액세스하는 것이 아니라 쉽게 재생성할 수 있기 때문에

⇒ 그리고 60일 후에 만료시키거나 삭제하는 라이프사이클 설정이 있을 수 있다.

### amazon S3 - lifecycle rules - 시나리오2

회사 규칙에 따라 30일동안은 삭제된 S3 객체를 즉각적으로 복구할 수 있어야 한다. 그 기간이 지나면 치장 365일 동안은 삭제된 객체를 48시간 이내에 복구할 수 있어야 한다.

→ 그렇게 하기 위해서 s3 버저닝을 활성화해서 객체 버전을 보관할 수 있어야 한다.

→ 삭제된 객체들은 실제로 삭제 마커에 의해 감춰져 있다. 그 다음에 복구할 수 있다.

→ 그 다음에 현재 버전이 아닌(= 최상위 버전이 아닌) 객체들을 standard IA로 이전하기 위한 규칙을 만들 것이다.

⇒ 현재 버전이 아닌 버전들을 아카이브화를 목적으로 glacier deep archive로 이전할 수 있다.

### amazon S3 analytics - Storage class analysis

어떻게 어떤 객체를 다른 클래스로 이전할 최적의 일수를 결정할 수 있을까?

→ amazon s3 analytics를 이용하면 편하다.

standard/standard ia에 관한 추천사항을 제시한다.

(one-zone-ia나 glacier와는 호환되지 않는다)

s3 버킷이 있고 추가로 s3 analytics가 실행되고 있다.

s3 analytics가 .csv 보고서를 생성하고 약간의 추천 사항과 통계를 제공할 것이다.

보고서는 매일 업데이트될 것이고 결과는 24~48시간 정도 걸릴 수 있다.

---

## S3 - Requester Pays 요청자 지불

일반적으로는 버킷 소유자가 버킷과 관련된 모든 s3 스토리지 및 데이터 전송 비용을 지불한다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0e50c9f9-d6b9-4369-b0d8-a40d14ac1451/772a65e0-4513-4325-8ac2-31441a246a10/Untitled.png)

첫번째 그림에서는

요청자, 즉 사용자가 버킷으로부터 파일을 다운로드한다.

네트워크 비용 역시 버킷 및 객체 소유자에게 청구된다.

만약 대형 파일이 있고 고객이 많이 다운로드한다면 요청자 지불 버킷을 활성화하면 된다.

버킷 소유자가 아닌 요청자가 객체 데이터 다운로드를 지불한다.

→ 요청자가 익명이어서는 안된다! (요청자가 aws에서 인증을 받아야 한다.)

대량의 데이터 셋을 다른 계정과 공유하려 할 때 유용하다.

---

### S3 event notification

s3에서는 이벤트가 발생할 것이다.

이벤트는 객체가 생성/삭제/복구/복제되는 것을 말한다.

→ 그리고 그런 이벤트들을 필터링 할 수 있다. (ex) *.jpg)

- 사용 사례

:s3에서 일어나는 특정한 이벤트에 자동으로 반응하려는 경우

ex) s3에 업로드된 모든 이미지의 섬네일을 생성하려 할 수 있다.

원하는 만큼 s3 이벤트를 만들 수 있다.

원하는 어떤 타킷에도 전송할 수 있다.

→ 이런 이벤트들은 몇 초 안에 대상으로 전달되지만 몇 분이 걸릴 수 있다.

### S3 event notifications - IAM permissions과 이벤트 알림 타깃

1. SNS

이벤트 알림이 작동하려면 IAM 권한을 갖고 있어야 한다.

s3서비스는 데이터를 SNS 토픽에 전송한다.

(이걸 하기 위해서는 SNS 리소스 정책이라는 것을 첨부해야 한다.

해당 정책은 SNS 토픽에 첨부하는 IAM 정책이다. s3 버킷이 SNS 토픽에 직접 메시지를 전송하도록 허용해준다.)

1. SQS

이와 비슷하게 SQS를 사용하는 경우 SQS 리소스 액세스 정책을 만들고 그것이 s3 서비스가 SQS queue에 데이터를 전송하도록 허가해준다.

1. lambda function

람다 리소스 정책을 람다 함수에 첨부하여 s3가 함수를 호출할 권한을 제공해야 한다.

⇒ s3가 아닌 SNS 토픽, SQS Queue, lambda function에서 리소스 액세스 정책을 정의한다.

이것들은 s3 버킷 정책을 사용할 경우와 비슷하게 작동한다.

### S3 event notifications with amazon EventBridge

이벤트는 s3 버킷으로 가고 모든 이벤트는 결국 amazon eventbridge로 가게 된다.

event bridge에서는 규칙을 설정할 수 있으며 그 규칙들 덕분에 이벤트들을 18가지의 aws 서비스에 전송할 수 있다.

→ s3 이벤트 알림의 능력이 크게 향상된다.

event bridge를 사용하면 훨씬 많은 **고급 필터링 옵션**을 선택할 수 있다.

(메타 데이터, 객체 사이즈, 이름을 필터링을 하고 **다수의 대상에 전송**할 수 있다)

event bridge에서 바로 제공하는 기능을 사용할 수 있다.

---

### S3 - baseline performance 기준 성능

기본적으로 s3는 요청이 아주 많을 때 자동으로 확장된다.

s3로부터 첫번째 바이트를 수신하는데 지연시간도 100~200밀리초 사이로 매우 짧다.

s3는 버킷 내에서 접두사당 초당 3500개의 put/copy/post/delete, 초당 5500개의 get/head 요청을 지원한다.

버킷 내에서 접두사 수에 제한이 없다.

네 개의 접두사에 읽기 요청을 균등하게 분산하면 초당 22000개의 get/head 요청을 처리할 수 있다.

### S3 성능과 최적화 방법

1. 멀티 파트 업로드

   00mb가 넘는 파일은 멀티파트 업로드를 사용ㅇ하는 것이 좋다.

   5gb가 넘는 파일은 반드시 사용해야 한다.

   업로드를 병렬화하므로 전송속도를 높여 대역폭을 최대화할 수 있다.

2. S3 전송 가속화 ( 업로드와 다운로드용)

   파일을 aws 엣지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 s3 버킷으로 전달한다.

   (엣지 로케이션은 리전보다 수가 많다)

   멀티파트 업로드와 같이 사용할 수 있다.

   퍼블릭 인터넷의 사용량을 최소화하고 프라이빗 aws 네트워크의 사용량을 최대화하는 것이다.


### S3 성능 - S3 byte-range fetches

S3 바이트 범위 가져오기라는 기능이 있다.

파일에서 특정 바이트 범위를 가져와서 get 요청을 병렬화한다.

특정 바이트 범위를 가져오는 데 실패한 경우에도 더 작은 바이트 범위에서 재시도하므로 실패의 경우에도 복원력이 높다.

사용사례 1. 다운로드 속도를 높일 때 사용한다.

모든 요청을 병렬화된다.

사용사례 2. 파일의 일부만 검색한다.

---

### S3 select & glacier select

S3에서 파일을 검색할 때 검색한 다음에 필터링하면 너무 많은 데이터를 검색하게 된다.

sql문에서 간단히 행과 열을 사용해 필터링할 수 있다.

네트워크 전송이 줄어들기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 CPU 비용도 줄어든다.

속도는 400% 빨라지고 비용은 80% 줄어든다.

---

### S3 batch operations

단일 요청으로 기존 S3 객체에서 대량 작업을 수행하는서비스이다.

→ 해당 작업으로 S3 버킷 간에 객체를 복사할 수 있다.

→ 암호화되지 않은 모든 객체를 암호화할 수 있다.

작업은 객체의 목록, 수행할 작업 옵션 매개 변수로 구성된다.

이를 사용하면 재시도를 관리할 수 있고 진행 상황을 추적하고 작업 완료 알림을 보내고 보고서 생성 등을 할 수 있다.

이 객체 목록은 어떻게 생성되냐면 S3 inventory라는 기능을 통해 가져온다.

그리고 s3 select를 사용해 객체를 필터링한다.